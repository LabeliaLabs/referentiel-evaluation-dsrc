# Atelier du 2 avril 2020 - Notes / Matière brute

_Présents : @ClementMayer, @bowni, @natct10, @SaboniAmine, @celinejacques, Anne-Sophie Cissey, Timothée Faucon, Cédric Meurée, Jeremie Abiteboul, Raphaelle Bertrand, Augustin Durivault, Anabelle Blangero, Timothé Delion, Grégory Chatel_

_Merci à tou.te.s !_

Retrouvez [les slides de la présentation](https://docs.google.com/presentation/d/1C46TOlvJidpKBekw8xjbqVU3q3h7kl85jfCfdb7x0kQ/edit).

## Introduction et contexte

### Labelia Labs (ex- Substra Foundation)

- [Présentation de l'association](https://frama.link/substrapres)
- [Repos Labelia Labs (ex- Substra Foundation)](https://github.com/SubstraFoundation/)
- [Projet Healthchain](https://www.labelia.org/fr/healthchain-project)
- [Projet Melloddy](https://www.labelia.org/fr/melloddy-project)
- _New_ ! **[Documentation du framework Substra](https://doc.substra.ai)**

### Démarche Data Science Responsable et de Confiance

- [Le dépôt](https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance)
- **[Le référentiel](https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance/blob/master/referentiel_evaluation.md)**

## Présentation aiVision et retour d'expérience

Présentation par Cédric Meurée, [aiVision](https://www.aivision.health/).

### Présentation aiVision

- Plateforme de télémédecine pour ophtalmologistes et orthoptistes
- Solutions basées sur de l'intelligence artificielle pour l'aide au diagnostic (rétinopathie diabétique, DMLA, glaucome)

### Pourquoi rejoindre les ateliers ?

- Domaines d'activité
- Convictions et intérêts personnels
- Croissance de l'équipe aiVision : bon moment pour s'interroger sur bonnes pratiques à mettre en place
- Visibilité et crédibilité pour l'entreprise : atout dans ce secteur d'activité

### Revue des mesures

Revue de deux mesures par thème (afin de ne pas parcourir tout le référentiel) et de l'évaluation de ces données.

L'évaluation est sur une échelle de 0 à 5 - Les niveaux de maturité :

- 0 : Mesure non implémentée / Pratique inexistante ou incomplète
- 1 : Mesure en cours d'implémentation / Pratique informelle
- 2 : Mesure implémentée nécessitant amélioration / Pratique répétable et suivie
- 3 : Mesure implémentée / Processus défini
- 4 : Mesure implémentée et contrôlée / Processus contrôlé
- 5 : Mesure implémentée, contrôlée et optimisée / Processus en amélioration continue

**Question :** Sur l'utilisation de dataset de données publiques : quelles informations sont disponibles sur comment ils ont été élaborés ? Manière de prendre les photos, technologies, époques... ?

Des informations sont prises sur le dataset, beaucoup sont récents avec de l'information sur comment ils ont été constitués.
Sur des dataset plus anciens, images segmentés à la main : moins d'informations disponibles.  

**Question :** L'âge des patients est-il disponible sur toutes les données ?

Pour les jeux de données publiques, ils viennent souvent d'Amerique du nord et d'Indes : information disponible sur l'éthnie par exemple mais pas nécessairement l'âge.

L'information est présente sur les dataset d'études cliniques nécessaires pour valider les dispositifs médicaux.
--> l'information est disponible plus facilement sur les datasets de test que d'entraînement.

**Fairness metrics (BIA-03) :**
Certaines mesures ne sont pas pertinentes en fonction du domaine de l'organisation. Certaines mesures sont rédigées de manière trop ambiguë et il n'est pas facile de s'évaluer.

**Question :** DON-3 - évaluation d'aiVision de 4/5 : êtes vous en capacité de déterminer ce qu'il vous faut faire pour passer d'un processus défini à un processus contrôlé ?

Aujourd'hui, l'accompagnement est extérieur pour le projet. Nécessité d'internaliser la compétence / de s'approoprier les outils. Trop de dépendance à l'accompagnement.

**Référence essentielle :** [Continous delivery for machine learning](https://martinfowler.com/articles/cd4ml.html)

Cette référence a été utilisée pour traiter un certain nombre de mesures.

**Processus vs. Formalisation :** Certaines mesures sont bien mises en place mais pas toujours documentées.

**Réutilisation de modèle (exemple : transfer learning) :** nécessité d'avoir la généalogie des modèles qui sont transférés. Forte complexité. Beaucoup de recherche et outil sur le sujet en cours.

**Question :** Avez-vous implémentés une boucle de feedback pour récupérer les retours des médecins/experts sur les prédictions?

En amont : validation de l'outil : besoin d'avoir plusieurs médecins qui notent les mêmes images puis croisement des résultats. Application de l'outil --> rapprochement entre le retour des médecins et l'outil.
Cela est obligatoire pour validation réglementaire du dispositif.

Une fois que le modèle existe et mis en prod : pas de vérification mise en place à ce stade.

**Référence intéressante :** Calcul de l'impact CO2 en fonction de la plateforme, de la région - [ML CO2 impact](https://mlco2.github.io/impact/)

### Intérêt de l'exercie d'auto-évaluation

**Points forts d'aiVision :**

- Démarrage dynamique CI/CD en ML : s'inscrire dans l'esprit des mesures de type généalogie des modèles
- Mesures les plus avancées : suivi de projets individuels

**Points d'amélioration :**

- Généraliser la documentation, au niveau de l'entreprise
- Identifier et définir plus clairement les chaînes de responsabilités

=> aiVision considère que le bilan est très positif pour eux.

**Question :** Votre modèle principal étant du deep learning, qu'en est-il de l'interprétabilité / explicabilité des décisions ?

Inclure les probabilités de décisions - fournir les probas par classe en plus du diagnostique
Mettre en place l'explication : quelle zone de l'image a aidé à faire le diagnostique.
Travail également sur des analyses d'images plus classiques

## Enseignements

L'outil est déjà utile !
Les points d'amélioration sont détaillés dans la présentation : certaines mesures vont être revues afin de résoudre ces différents problèmes.

## Prochaines étapes et ouverture

_D'autres organisations souhaitent s'auto-évaluer ? [Dites-le-nous !](mailto:hello@substra.org)_

Retour d'aiVision sur l'auto-évaluation : cela permet de prendre du recul, de voir où on en est.

**Temps pour réaliser l'évaluation :** environ une demi-journée  (vs x10h sur le référentiel de l'initiative de l'Union Européenne !)

La plateforme de self-assessment est en cours de travail.

**Références à exlorer :**

- Initiative ["Model Cards"](https://modelcards.withgoogle.com/about) de Google.
Souvenir de toutes les étapes de la création d'un modèle --> à creuser pour les ressources.

- [Dataset Version Control](https://dvc.org)
Versioning de vos performances / datasets
Outil de versioning de dataset pour la data science.
Tagguer certains modèles en fonction des datasets utilisés.

**Question :** Les outils du framework open source Substra permettent-ils "par construction" d'obtenir de bonnes cotations selon le référentiel ? Quelles dimensions en particulier ?

Framework Substra : une des ressources parmi les nombreuses utiles pour les approches responsable et de confiance :

- Tracabilité complète de toutes les opérations dans les projets : généalogie de bout en bout, historique intéressant pour l'audit / certification
- Protection de la confidentialité de la donnée : projet de DS sans que les données ne bougent, uniquement les modèles et les algos. Risque pas zéro mais diminution de la surface d'exposition et de la nature du risque.
- Performance : cadrage de la performance, nécessité de rentrer sa metric de performance + la séparation training test. Plus compliquer de p-hacker / data dredging.

**Merci à tou.te.s**

_[Inscrivez-vous](www.labelia.org) à notre newsletter pour ne pas manquer la prochaine édition !_

---
_Logs du tchat pendant l'atelier :_

Nathanaël Cretin14:04
Bonjour tout le monde

Clément Mayer14:04
Hello :)

GREGORY CHATEL14:06
bonjour à tous

Clément Mayer14:06
Salut Grégory, on démarre dans une minute

Clément Mayer14:10
C'est par içi !

Nathanaël Cretin14:12
La prise de note est par ici : https://video.etherpad.com/p/9fv0-adsrc3?lang=fr

Nathanaël Cretin14:15
Les depots Github : https://github.com/SubstraFoundation

Nathanaël Cretin14:19
Le dépot du projet DSRC : https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance

Nathanaël Cretin14:20
Documentation : https://substrafoundation.github.io

Nathanaël Cretin14:27
aiVision : https://www.aivision.health/

Vous14:28
Est-ce qu'il y a des questions pour Cédric sur aiVision ?

Annabelle Blangero14:29
c'est très clair

Vous14:29
Ok !

Nathanaël Cretin14:31
Le référentiel : https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance/blob/master/referentiel_evaluation.md

Vous14:34
Les niveaux de maturité :
0 : Mesure non implémentée / Pratique inexistante ou incomplète
1 : Mesure en cours d'implémentation / Pratique informelle
2 : Mesure implémentée nécessitant amélioration / Pratique répétable et suivie
3 : Mesure implémentée / Processus défini
4 : Mesure implémentée et contrôlée / Processus contrôlé
5 : Mesure implémentée, contrôlée et optimisée / Processus en amélioration continue

Amine Saboni14:35
Pouvez vous déterminer quelles seraient les étapes pour arriver à 4/5?
4 sur 5
*

Clément Mayer14:37
je note la question pour en rediscuter dans la partie enseignement

Annabelle Blangero14:40
Tous les ages de patients dans les données?

GREGORY CHATEL14:40
Il me semble qu'il est possible de déterminer plusieurs informations démographique comme le genre, l'age, le body mass index. Un modèle pourrait apprendre à discriminer selon ces critères. Avez-vous pris ce genre d'information en compte dans votre étude ?

GREGORY CHATEL14:47
merci pour la réponse

Clément Mayer14:49
https://martinfowler.com/articles/cd4ml.html
Continuous Delivery for Machine Learning

Annabelle Blangero14:50
merci pour le lien

Annabelle Blangero14:53
Vous avez implémentés une boucle de feedback pour récupérer les retours des médecins/experts sur les prédictions?

GREGORY CHATEL14:56
À quel point cette démarche de suivi de la construction du modèle et des dataset d'entrainement est couverte par l'initiative "Model Cards" de Google ? (https://modelcards.withgoogle.com/about)

Nathanaël Cretin14:56
Très clair !

Annabelle Blangero14:57
oui pour l'instant :) mais c'était aussi pour la mise en prod

GREGORY CHATEL14:57
(on peut aborder ma question plus tard durant les discussions, c'est un petit peu spécifique)

Annabelle Blangero14:58
ok!

Amine Saboni14:58
Même question que Grégory pour le versioning de vos performances / datasets : https://dvc.org

Nathanaël Cretin14:59
Looks good!

Vous14:59
Ok, merci pour ces références. Prenons un temps pour que vous en disiez chacun un mot tout à l'heure

Clément Mayer15:03
https://mlco2.github.io/impact/. mesure de l'impact du machine learning sur le CO2

Amine Saboni15:09
Bravo pour cet effort d'auto évaluation, ces retours sont très intéressants, merci à vous

Céline Jacques15:09
Bravo!! Super retour d'expérience !

Annabelle Blangero15:09
Votre modèle principal étant du deep learning, qu'en est-il de l'interprétabilité / explicabilité des décisions ?

Annabelle Blangero15:13
ok! merci

Tim F15:13
Merci Cédric!

Clément Mayer15:13
Merci Cédric !

Nathanaël Cretin15:13
Super présentation !

Nathanaël Cretin15:31
Du coup, combien de temps dure une évaliation ?
*évaluation

Ok !

Timothé Delion15:33
Les outils du framework open source Substra permettent-ils "par construction" d'obtenir de bonnes cotations selon le référentiel ? Quelles dimensions en particulier ?

Amine Saboni15:37
Pour préciser ma question sur votre évaluation du DON-3 : êtes vous en capacité de déterminer ce qu'il vous faut faire pour passer d'un processus défini à un processus contrôlé ?

Nathanaël Cretin15:38
pas spécialement de mon côté

Amine Saboni15:40
Ok, merci

GREGORY CHATEL15:42
Merci pour l'organisation !

Nathanaël Cretin15:42
Vivement que l'on se retrouve irl

Annabelle Blangero15:42
c'était très intéressant merci beaucoup. Ma connection aujourd'hui n'est pas suffisamment bonne pour prendre la parole, mais on aura d'autres occasions :)

Cédric Meurée15:42
Merci de nous avoir proposé ce 1er test, et pour les questions

Jeremie Abiteboul15:43
merci pour l'organisation, et en espérant un atelier "physique" en juin

Nathanaël Cretin15:43
Pensez à mettre des étoiles sur Github \o/

Timothé Delion15:43
Merci beaucoup !

Amine Saboni15:43
Merci à vous, à bientôt !

Tim F15:43
Merci à toute l'équipe
