# Atelier DSRC S02E01 du 22 février 2021

Présents : @ClementMayer, @bowni, @natct10, Antoine Duval, Alice Gasselin, Ana Ulianovici, Antoine Duval, Camille, Camille Boivigny, Cédric Meurée, Céline Jacques, Charlène Dupont, Cléo Tassain, Daniel Bartolo, Edwige Zhu, Eugénie Clément, Eric Féjan, Fabien Faivre, Gabrielle Du Marais, Gabrielle Rives, Gregor Baues, Helene Jeannin, Heytem Boumaza, Jeremie Abiteboul, Joséphine Lecoq-Vallon, Julie Bec, Laurence Godnair, Line Ton That, Lee Clementine, Luis Arias, Michael Fine, Minwei Deng, Mylan Deveau, Nathan Lauga, Nicolas Landel, Pierre Delanoue, Rahali Bilel, Salaheddine Jouhri, Sebastien Quinault, Soufiane Abouliatim, Thomas Bouché, Véronique Macé, Yann Golhen.

## Liens utiles

- [Présentation](https://data-for-good.slack.com/archives/C04GWR7J8/p1614077106056700)
- [Plateforme d'auto-évaluation](https://assessment.substra.ai/)
- [Référentiel d'évaluation](https://github.com/SubstraFoundation/referentiel-evaluation-dsrc/blob/master/referentiel_evaluation.md)
- [Article de Blog sur les Fairness metrics](https://www.substra.ai/en/blog/fairness-in-machine-learning)
- [Outil Open Source Shapash](https://github.com/MAIF/shapash)

## Sommaire

- Introduction à ce nouveau cycle d’ateliers
- Revue de l'article de blog sur la Fairness
- Retour d’expérience sur l'évaluation par la MAIF
- Présentation de l'outil Open Source Shapash développé par la MAIF
- Coup d’oeil sur 2 initiatives : “Responsible AI? report” et (R)REPEATS
- Enseignements du guide Impact IA : "Guide IA digne de confiance, pour construire une gouvernance adaptée à chaque entreprise "
- Présentation des nouveautés sur la plateforme et définition d'un plan de lancement
- Amélioration itérative du référentiel d’évaluation
- Gouvernance du référentiel d’évaluation

## Retour d’expérience sur l'évaluation par la MAIF par Daniel Bartolo

Présentation du contexte de la  MAIF :

- Modèle mutualiste
- Signataire de la Charte du numérique au service de l'homme
- Démarche interne pour IA plus éthique (sensibilisation interne & élaboration cadre éthique à l'élaboration des projets d'IA à partir des travaux du HLEG - high level expert group de l'Union Européenne sur l'IA)

Le référentiel d'évaluation semble complémentaire avec les démarches initiées et cohérent avec les travaux européen mais adapté au contexte français.
3 personnes ont participé à l'évaluation

**Retour** 

Points d'intéret du référentiel d'évaluation :

- périmètre entreprise / organisiation
- beaucoup de ressources documentaires (aide aussi à bien comprendre les questions)
- baromètre du niveau de maturité
- permet d'objectiver les avancées et demain mesurer les évolutions
- permet d'identifiier clairement les axes de progrès et avoir une vision globale
- Pourrait devenir un outil de veille et d'anticipation
- léger et "frugal" (ne demande pas un trop grand effort pour avoir un premier retour)
- questionnaire généraliste (pas spécialisé sur un secteur particulier)
- travail synthétique d'autres éléments (résonne avec d'autres travaux)

Points d'amélioration :

- aujourd'hui, la démarche est non certifiante et déclarative

-> sujet de travail pour cette année. Aujourd'hui, l'évaluation est gratuite et libre accès pour travailler et progresser en interne mais pas pour communiquer en externe. Un audit permettra de faire de la communication dessus en externe (travail en cours)

- Une visualisation plus graphique serait appréciable

-> prévue dans la prochaine livraison !

## Présentation de Shapash

Projet open source [Shapash](https://github.com/MAIF/shapash) :

- outil visant l'explicabilité des modèles de Data Science, orienté grand public
- surcouche de SHAP, LIME
- génèse : rendre partageable les résultats pour les clients - objectif de donner de l'explicabilité auprrès des clients finaux
- [Publication](https://pub.towardsai.net/shapash-making-ml-models-understandable-by-everyone-8f96ad469eb3)

Deux options :

- en mode exploration -> participer à l'élaboration des modèles (object SmartExplainer)
- en mode production -> suivre les modèles (SmartPredictor)

Roadmap :

- output html report (+ de précision dans les graphes entre autrre)
- intégration de nouvelles méthodes (backend update, alternative à SHAP et Lime)
- webapp : Aggrégation de features & interactions (zoom et navigation dans les features)
- NLP

Aujourd'hui, un cas d'usage en production avec la possibilité pour un conseiller :

- d'introduire son discours commercial
- de déjuger les recommandations

**N'hésitez pas à fournir des feedback autour de ce projet !**
**Vous pouvez également contribuer à ce projet dont la roadmap ouverte !**

## Sujets de travail sur le référentiel d'évaluation

Quelques sujets actuellement ouverts ont été mentionnés, pour inviter les participants à y contribuer :

- lien vers les [issues ouvertes](https://github.com/SubstraFoundation/referentiel-evaluation-dsrc/pull/121/files)
- Intégration des ressources élaborées dans le cadre de Data For Good
- Biais et fairness : split de l’élément d’évaluation 2.3 afin de créer un élément spécifique et plus complet sur la fairness
- 3 sujets pas encore explicitement traités dans le référentiel d’évaluation :
  - Egalité Femmes-Hommes & Diversité
  - "AI Literacy"
  - Accessibilité
- Intégration suite aux enseignements du Guide Impact AI
- Ajouter des "hints" aux quelques éléments d’éval. qui n’en ont pas encore

## Gouvernance du référentiel d’évaluation

Dans la perspective de la création d'un label, Labelia Labs (ex- Substra Foundation) lance la mise en place d'une gouvernance plus formelle du référentiel :

- appel à manifestation d'intérêt
- mise en place d'un comité de validation des nouvelles versions (2 à 4 par an)
- objectif : constitution d'un comité de 7 à 10 personnes

**Merci à tous pour votre participation !**
