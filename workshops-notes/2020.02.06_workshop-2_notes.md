# Atelier du 6 février 2020 - Notes / Matière brute

_Présents : @ClementMayer, @bowni, @natct10, @mattthieu, @SaboniAmine, @celinejacques, Anne-Sophie Cissey, Véronique Brun, Timothée Faucon, Cédric Meurée, Paul Dalous, Jeremie Abiteboul_

_Merci à tous !_

> **Note :** la version étudiée durant l'atelier est celle du [commit 57e07dd3e588c936a73d5e4d2b1ddae5d5ba12a1](https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance/commit/57e07dd3e588c936a73d5e4d2b1ddae5d5ba12a1). La numérotation des risques et mesure a évolué depuis.

## 1. Retour sur l'actualité

### Actualité n°1 : Ethique vs. Responsable et de Confiance

Discussion autour de quelques articles sur ce thème :

- [Article de Tom Chatfield](https://onezero.medium.com/theres-no-such-thing-as-ethical-a-i-38891899261d)
- [Post Linkedin "IA et éthique : CALMONS-NOUS"](https://www.linkedin.com/pulse/ia-et-%C3%A9thique-calmons-nous-jerome-fortias/)
- [Post Linkedin "IA et éthique ? Ou plutôt responsable et de confiance"](https://www.linkedin.com/pulse/ia-et-%C3%A9thique-ou-plut%C3%B4t-responsabilit%C3%A9-confiance-eric-boniface/)

Certains praticiens ne semblent pas apprécier la profilération d'articles sur l'éthique et l'IA. Utilisation du terme éthique problématique : pas d' "IA éthique" mais des questions qui devraient porter sur les usages. Certains suggèrent d'accepter une approche boite noire et de concentrer les travaux normatifs sur les usages.

Qu'est-ce qu'être éthique, peut on en choisir une définition ? Plusieurs définitions de l'éthique :

- Usage ? bien / pas bien
- Grands principes
- Objectifs moraux

[Lecture sur l'éthique](https://www.erudit.org/fr/revues/ltp/1996-v52-n2-ltp2155/401006ar.pdf) (référence à Max Weber).

Retour d'expérience sur une conférence de l'[OSCE](https://www.osce.org) sur la sécurité européenne : difficulté de comprendre, caractériser, définir ce qu'est l'IA, ce qu'en sont les usages. Le risque est alors de ne pas s'y intéresser et y travailler suffisamment (au regard des opportunités et menaces soulevées).

Quelle est la nouveauté, la singularité de ces technologies ? Le point qui semble vraiment nouveau est l'utilisation de modèles "appris" vs. jusqu'alors des systèmes bâtis sur des règles explicitement définis. Des problèmes se posent alors en particulier sur la gouvernance et les responsabilités.

_Analogie_ : quid quand **mauvaises prévisions météo** ? quelle est la responsabilité, sait-on se retourner vers un modèle ?

Deux exemples forts :

- Apple Card et le plafond autorisé différencié entre homme et femme dans des couples avec historique bancaire commun --> problème : qui est responsable ? Pendant plusieurs jours qui ont suivi le début de la crise pas de réponse : Apple renvoyant vers son sous-traitant Goldman Sachs, Goldman Sachs ne sachant trop que répondre et défendant son modèle.
- [COMPAS](https://en.wikipedia.org/wiki/COMPAS\_(software)) : système sur liberté conditionnelle / récidive - problème sur les métrique de fairness.

Cas de [la police de Chicago](http://www.mutationstechnologiques.fr/la-mecanique-algorithmique-de-la-police-predictive-a-chicago/) qui a complétement débranché les algos utilisés au bout de plusieurs années.  

Analogie intéressante : plan de vol / condition d'utilisation d'un appareil.

**Modèle d'IA** : aujourd'hui abordable en apparence par des non-spécialistes - Tensor Flow permet d'obtenir rapidement de bonnes performances --> mais quid de l'identification de biais ? Comment cela doit-il se faire ? Il y a peu de recul sur ce qui est fait (pas de distance critique). Fossé entre la facilité d'utilisation en apparence, pour démarrer, et la complexité induite de ces technologies ML pour une organisation qui voudrait les utiliser en prise avec ses parties prenantes.

_Analogie_ : sécurité informatique : télécharger un logiciel pratique vs. se préoccuper des failles de sécurité qu'il peut ouvrir, de sa mise à jour dans le temps, du coût des licences, de la dépendance qu'il peut créer, de la capacité à l'interconnecter avec d'autres processus --> nécéssité de mettre en place des processus / un référentiel de bonnes pratiques.

**Data Science :** technique ok. Maintenant il est temps de mettre en place un environnement autour pour mettre de la confiance.

_[pas clair - à préciser]_ Problème d'esprit critique autour de l'informatique / IA vs. bienfait de l'IA : persistence (pas de sentiments)

Problème des systèmes fermés :

- Exemple si problème, pas de possibilité de changer dans le système
- prise en compte de l'*input* humain face à une décision automatisée. **Mesure potentielle : pouvoir aller contre l'IA ?**

Note : le terme "Responsable" est connu et pratiqué par les organisations, est un concept juridique.

### Actualité n°2 : Google s'intéresse à la régulation

Intérêt pour de la régulation côté Google, cf. récent plan media du CEO Sundar Pichai.

Faut-il être un peu méfiant ? Activité habituelle de lobby.

Réflexion : il existe un gap entre RGPD 2012 et 2018 --> ce qui a été décidé n'est plus valable. Risque d'avoir le même problème pour l'IA entre les lobbies + les technologies dépassées.

### Actualité n°3 : ISO s'attaque à l'IA

[Article](https://blog.iec.ch/2019/11/international-standards-committee-on-ai-ecosystem-achieves-milestone-and-launches-new-areas-of-study/)

ISO : très complexe en terme de méthode, calendrier, découpage en thèmes.
Timing visiblement très long de leur côté --> possibilité de cohabiter.

ISO : plutôt suivi et documentation ? approche pas forcément responsable, à creuser.

En attente, pas d'actions identifiées à ce stade.

### Actualité n°4 : Health Data Hub

[Les avocats en guerre](https://www.cnb.avocat.fr/sites/default/files/11.cnb-mo2020-01-11\_ldh\_health\_data\_hubfinal-p.pdf)
Problème lié à Azure / Microsoft et le Cloud Act. Illustre la véhémence des attaques publiques et le besoin de confiance.

### Actualité n°5 : Proposition de loi sur la charte de l'IA et des algorithme

[Loi sur la charte de l'IA et des algorithmes](http://www.assemblee-nationale.fr/15/pdf/propositions/pion2585.pdf). Un point en particulier sur la notion de responsabilité est intéressant :
> « Un système tel que défini au précédent alinéa n’est pas doté de la
personnalité juridique et par conséquent inapte à être titulaire de droits
subjectifs. Cependant les obligations qui découlent de la personnalité
juridique incombent à la personne morale ou physique qui héberge ou
distribue ledit système devenant de fait son représentant juridique.

### Actualité n°6 : Etalab

Rapport sur la [responsabilité des algorithmes publics](https://www.etalab.gouv.fr/algorithmes-publics-des-eleves-de-lena-formulent-une-serie-de-recommandations-sur-les-enjeux-dethique-et-de-responsabilite).

## 2. Revue des risques

### Partie 1 : EDP

**EDP-01** - Définition à donner : _Données confidentielles_ : données qui ont de la valeur mais pas nécessairement personelles (exemple : pharma.)

**EDP-01** - [Ré-identification, article Nature](https://www.nature.com/articles/s41467-019-10933-3/) : "Using our model, we find that 99.98% of Americans would be correctly re-identified in any dataset using 15 demographic attributes."

Obfuscation > Anonymisation

**EDP-04** - Elargir pas que changement de réglementation mais par exemple double exposition. Exemple de contradiction entre les différentes réglementations : [FATCA (Foreign Account Tax Compliance Act) contredit RGPD](https://www.cnil.fr/fr/cnil-direct/question/loi-fatca-que-faire)

**EDP-01** - Exposer : qu'est-ce que cela signifie ?

Précision vs. viser large ? réponse pas évidente.
Précision = difficulté d'évaluer au niveau d'une organisation.
--> voir au niveau des mesures.

**EDP-01** - Trade-off entre exposition vs. et traçabilité/généalogie
Illustration intéressante : mise en place d'un [traceur](https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/) par Facebook sur les données utilisées par un algo.

L'utilisateur doit être informé de l'utilisation de ses données - rendre des comptes.

### Partie 2 : PDI

**PDI-03** - La manière dont le modèle va être construit peut entrainer des biais.

- doc2vec --> voir [issue](https://github.com/SubstraFoundation/referentiel-ds-responsable-confiance/issues/25) créé par @SaboniAmine
- Autre exemple : features / intégration de l'ethnie

_Social cooling_ : auto-censure des utilisateurs de réseaux sociaux sachant que ceux-ci sont utilisés pour des études / de la surveillance, etc. Cela fait que les interactions sur les réseaux sociaux deviennent moins représentatives pour des études.

**PDI-04** - Empoisonnement de données --> cas à identifier

**PDI-06** - Modèle drift :

- exemple à trouver dans le trading
- Maintenance prédictive : problème sur des capteurs
- --> drift modèle vs. drift données - séparer en deux cas ?

Autres exemples sur la partie PDI :

- [Face ID / reconnaissance faciale marche mal en Chine](https://www.thesun.co.uk/news/5182512/chinese-users-claim-iphonex-face-recognition-cant-tell-them-apart/)
- [biais dans les systèmes de reconnaissance visuelle](https://arxiv.org/pdf/1902.11097.pdf)

### Partie 3 : RC

Réflexion sur les distinctions à faire dans cette partie entre les modèles créés par apprentissage supervisé vs. par renforcement ? Le "plan de vol" est certainement plus difficile à définir / décrire dans les cas d'apprentissage par renforcement.

LinkedIn : utilisation d'un algo qui peut discriminer sans le savoir. + auto-optimisation - manipuler des algos.
--> Manipulation / exploitation d'un algo à ajouter dans les risques

Responsabilités au sein-même d'une organisation ? --> partie prenante interne à une organisation à identifier (pas que vis-à-vis de l'extérieur)

### Partie 4 : ESE

**ESE-01**

Trade-off: Entrainement à l'origine vs. entrainement à chaque fois --> en effet, un entrainement unique peut être meilleur mais il faut bien en avoir conscience  / communiquer.

IA justifié vs. non justifié ? Réglementations à venir ? À l'avenir systèmes non autorisés ?

Problème sur l'emploi - exemples :

- La Redoute sur les fiches produits
- traders au chômage
- [En attendant les robots d'A. Cassili](http://www.seuil.com/ouvrage/en-attendant-les-robots-antonio-a-casilli/9782021401882)

Fermes à clics : impacts sociaux pas seulement du fait de l'utilisation de systèmes automatiques, mais aussi en amont notamment sur les données nécessaires à l'essor de ces technologies.
Travail des personnes / captcha --> travail déguisé.

**ESE-02**

- OpenAI : [GPT-2](https://openai.com/blog/gpt-2-1-5b-release/)
- Deepfake

Rajouter Cambridge Analytica comme exemple.

### Partie 5 : TR

Gouvernance globale : [intelligent organisation](https://www.amazon.fr/Intelligent-Organisation-Realising-value-information/dp/1138847070)

Article [Evgeny Morosov](https://slate.com/author/evgeny-morozov)

### Divers

Algorithme développé par des gens qui deviennent référence : si jamais un biais est présent il est difficile à identifier par les utilisateurs car ils ne sont pas les concepteurs.
Par ailleurs, les "grands modèles" qui deviennent des références (e.g. ResNet, BERT) pourraient présenter le risque d'être des single point of failure.

Impossibilité de mettre en place un modèle - exemple : EDF - problème sur la démonstration car impossible à tester en réel.

Notion de [Continuous delivery for machine learning](https://martinfowler.com/articles/cd4ml.html) à explorer et creuser.

Risque réglementaire : Pas suffisamment s'interroger sur la réglementation (exemple dans le monde bancaire nécessité de faire des choses reproductible + explications) --> à intégrer dans la première partie.

Transfert de performance de dataset à un autre : exemple publier un papier sur un dataset donné / **notion de robustesse d'un modèle** --> responsabilité ?

## Revue des mesures

### T1 : Protéger les données personnelles ou confidentielles

OWASP du Machine Learning ?
[OWASP Top Five ML risks](https://github.com/OWASP/Top-5-Machine-Learning-Risks/blob/master/Top%205%20Machine%20Learning%20Risks.md)

**Mesure à ajouter :** Est-ce que j'ai besoin de collecter des données personnelles ? minimisation des données, y compris quitte à détériorer la performance.

**Mesure à ajouter :** collaboration avec les autorités (e.g Anssi)

Suggestions de lecture : Newsletter [The ML Engineer](https://ethical.institute/index.html)

Idée à creuser : points IV et V à Rassembler ?

### T2 : Prévenir les biais malencontreux

Biais de genre : [Invisible Women: Exposing Data Bias in a World Designed for Men](https://www.amazon.fr/Invisible-Women-Exposing-World-Designed/dp/1784742929/)

Fairness : aussi au niveau individuel ? se rapproche d'une notion de robustesse --> "pour quelqu'un de très proche le résultat doit être très proche".

Biais : pas forcément négatif - des biais peuvent être exploités de manière positive. _[besoin d'exemples]_

Données synthétiques : manque le risque associé.

### T3 : Evaluer la performance de manière rigoureuse et expliquer les prédictions

_[pas clair, à désambiguïser]_ i : point d'attention - collaboration - risque d'overfit quand les tests ne sont pas divisés de la même manière.

Risque de contamination - deux points différents :

- Quand on a fait "l'erreur" : comment identifier ? Généalogie dans les travaux.
- Quand on a la donnée à deux endroits, dans les contextes de ML distribué / collaboratif

Pluralité de la base des tests ? (pour avoir des "backups")

iv - pas que apprentissage continu.

Modèle en production : même sur modèle stable, possibilité d'avoir des données qui ne sont plus dans le domaine (population & distribution). exemple : variable plus renseignée à la même fréquence qu'avant par les utilisateurs dans un SI

CI/CD pipeline sur les données (+ versioning).

Suivre si entre mise en place du modèle et actuel on est toujours bon.

Mesure le PSI : distribution des scores reste stable dans le temps.

Ajout de contrôle aléatoire humain - conforme à ce qui est attendu.
Mais quid quand trop ? --> utilisation d'autres mesures / combinaison entre plusieurs vérifications.

Mise en place de seuils d'indécision (voir plus loin)

Aborder les questions à chaque nouvelle application, pas que au moment du design.

Problème à vérifier : est-ce que le test doit évoluer ou non ?

### T4 : Etablir et maintenir une généalogie des modèles

Notion de "Model lineage" et [Data lineage](https://fr.wikipedia.org/wiki/Data\_Lineage) à creuser.

Chaque acteur doit avoir un outil d'interpretabilité spécifique.

Ajouter les limites d'utilisation en corollaire.

### T5 : Garantir la chaîne de responsabilité des modèles

Création d'un registre des modèles prédictifs (analogie avec le registre des traitements dans le RGPD) - (risque de perte de maîtrise) - Evaluation des risques (similaire au Privacy Impact Assessment RGPD).

Coupler ii et iii + v

iv : Chaine de responsabilité ?

- indépendance ?
- Modèle DPO ?
- Officer + pour chaque modèle un contact ?
- séparer entre :
  - Métier (application)
  - Data Scientist (biais)
  - sponsors ? Commanditaire ?

Responsabilité : possibilité de s'inspirer de la propriété intellectuelle ? _analogie à voir_

Etudier les licenses de distribution de modèle ML.

### T6 : Anticiper, suivre et minimiser les externalités négatives de l'activité data science

A découper - environnemental vs. sociétal (quanti vs. quali)

Explorer ce qui est dit dans le cadre de la Responsabilité sociétale et environnementale des entreprises (loi PACTE)

échelle d'Octo :

- compréhensible ou pas ? (pour Octo, outil différents explicabilité vs. interprétabilité)
- manière dont cela est présenté ?

Interprétabilité - complexe mais importante car demande sociétale. Il sera probablement nécessaire de démarrer avec une mesure sur le fait d'être conscient et d'essayer, et de la raffiner/sophistiquer dans le temps.
